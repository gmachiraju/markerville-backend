{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/gmachiraju/py2_kernel/lib/python2.7/site-packages/matplotlib/font_manager.py:273: UserWarning: Matplotlib is building the font cache using fc-list. This may take a moment.\n",
      "  warnings.warn('Matplotlib is building the font cache using fc-list. This may take a moment.')\n",
      "/Users/gmachiraju/py2_kernel/lib/python2.7/site-packages/matplotlib/__init__.py:1357: UserWarning:  This call to matplotlib.use() has no effect\n",
      "because the backend has already been chosen;\n",
      "matplotlib.use() must be called *before* pylab, matplotlib.pyplot,\n",
      "or matplotlib.backends is imported for the first time.\n",
      "\n",
      "  warnings.warn(_use_error_msg)\n",
      "/Users/gmachiraju/py2_kernel/lib/python2.7/site-packages/bs4/__init__.py:181: UserWarning: No parser was explicitly specified, so I'm using the best available HTML parser for this system (\"lxml\"). This usually isn't a problem, but if you run this code on another system, or in a different virtual environment, it may use a different parser and behave differently.\n",
      "\n",
      "The code that caused this warning is on line 162 of the file /System/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/runpy.py. To get rid of this warning, change code that looks like this:\n",
      "\n",
      " BeautifulSoup([your markup])\n",
      "\n",
      "to this:\n",
      "\n",
      " BeautifulSoup([your markup], \"lxml\")\n",
      "\n",
      "  markup_type=markup_type))\n"
     ]
    },
    {
     "ename": "IOError",
     "evalue": "[Errno 2] No such file or directory: 'databases/abbreviations.com.pkl'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIOError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-1d7ee43f1f27>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmatchers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mLF\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mcandidate_adjective_fixer\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mload_external_annotations_new\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mload_external_labels\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/gmachiraju/Box Sync/Gautam's Externally Shareable Files/Computer Backup/Documents/MallickLab/Petrel/markerville-backend/markerville-backend/LF.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[0mknowAbbreviations\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 43\u001b[0;31m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'databases/abbreviations.com.pkl'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     44\u001b[0m     \u001b[0mknowAbbreviations\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcPickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIOError\u001b[0m: [Errno 2] No such file or directory: 'databases/abbreviations.com.pkl'"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from six.moves.cPickle import load\n",
    "import cPickle\n",
    "import numpy as np\n",
    "\n",
    "from snorkel import SnorkelSession\n",
    "from snorkel.parser import XMLMultiDocPreprocessor, CorpusParser\n",
    "from snorkel.models import Document, Sentence, Candidate, candidate_subclass\n",
    "from snorkel.candidates import Ngrams, CandidateExtractor\n",
    "from snorkel.viewer import SentenceNgramViewer\n",
    "from snorkel.annotations import LabelAnnotator, load_gold_labels, FeatureAnnotator, save_marginals, load_marginals\n",
    "from snorkel.learning import SparseLogisticRegression, GenerativeModel, RandomSearch, ListParameter, RangeParameter\n",
    "from snorkel.learning.structure import DependencySelector\n",
    "from snorkel.learning.utils import MentionScorer\n",
    "from snorkel.contrib.rnn import reRNN\n",
    "\n",
    "import matchers\n",
    "import LF\n",
    "from candidate_adjective_fixer import *\n",
    "from load_external_annotations_new import load_external_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#------------------\n",
    "# Helper Functions\n",
    "#------------------\n",
    "\n",
    "def grabCandidates(extractor, schema):\n",
    "    # Candidate Counts\n",
    "    for k, sents in enumerate([train_sents, dev_sents, test_sents]):\n",
    "        extractor.apply(sents, split=k)\n",
    "        print \"Number of candidates: \", session.query(schema).filter(schema.split == k).count()\n",
    "\n",
    "    train_cands = session.query(schema).filter(\n",
    "        schema.split == 0).all()\n",
    "    dev_cands = session.query(schema).filter(\n",
    "        schema.split == 1).all()\n",
    "    test_cands = session.query(schema).filter(\n",
    "        schema.split == 2).all()\n",
    "\n",
    "    return [train_cands, dev_cands, test_cands]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#-----------------------\n",
    "# Setup & Preprocessing\n",
    "#-----------------------\n",
    "\n",
    "# Instantiate the Session\n",
    "session = SnorkelSession()\n",
    "\n",
    "\n",
    "# Doc Preprocessing\n",
    "file_path = 'articles/training.xml'\n",
    "train_preprocessor = XMLMultiDocPreprocessor(\n",
    "    path=file_path,\n",
    "    doc='.//article',\n",
    "    text='.//front/article-meta/abstract/p/text()',\n",
    "    id='.//front/article-meta/article-id/text()'\n",
    ")\n",
    "\n",
    "file_path = 'articles/development.xml'\n",
    "dev_preprocessor = XMLMultiDocPreprocessor(\n",
    "    path=file_path,\n",
    "    doc='.//document',\n",
    "    text='.//passage/text/text()',\n",
    "    id='.//id/text()'\n",
    ")\n",
    "\n",
    "file_path = 'articles/testcorpus.xml'\n",
    "test_preprocessor = XMLMultiDocPreprocessor(\n",
    "    path=file_path,\n",
    "    doc='.//document',\n",
    "    text='.//passage/text/text()',\n",
    "    id='.//id/text()'\n",
    ")\n",
    "\n",
    "# Parsing\n",
    "corpus_parser = CorpusParser()\n",
    "\n",
    "# Note: Parallelism can be run with a Postgres DBMS, but not SQLite\n",
    "corpus_parser.apply(list(train_preprocessor))\n",
    "corpus_parser.apply(list(dev_preprocessor), clear=False)\n",
    "corpus_parser.apply(list(test_preprocessor), clear=False)\n",
    "\n",
    "\n",
    "# Retrieving Stable IDs for each of the candidate sentences\n",
    "with open('articles/doc_ids.pkl', 'rb') as f:\n",
    "    train_ids, dev_ids, test_ids = load(f)\n",
    "\n",
    "train_ids, dev_ids, test_ids = set(train_ids), set(dev_ids), set(test_ids)\n",
    "train_sents, dev_sents, test_sents = set(), set(), set()\n",
    "docs = session.query(Document).order_by(Document.name).all()\n",
    "\n",
    "\n",
    "# Assigning each sentence to {train,dev,test}-set based on Stable ID\n",
    "for i, doc in enumerate(docs):\n",
    "    for s in doc.sentences:\n",
    "        if doc.name in train_ids:\n",
    "            train_sents.add(s)\n",
    "        elif doc.name in dev_ids:\n",
    "            dev_sents.add(s)\n",
    "        elif doc.name in test_ids:\n",
    "            test_sents.add(s)\n",
    "        else:\n",
    "            raise Exception(\n",
    "                'ID <{0}> not found in any id set'.format(doc.name))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#----------------------\n",
    "# Candidate Extraction\n",
    "#----------------------\n",
    "\n",
    "# Defining the Candidate Schemas\n",
    "BiomarkerCondition = candidate_subclass(\n",
    "    'BiomarkerCondition', ['biomarker', 'condition'])\n",
    "BiomarkerDrug = candidate_subclass('BiomarkerDrug', ['biomarker', 'drug'])\n",
    "BiomarkerMedium = candidate_subclass(\n",
    "    'BiomarkerMedium', ['biomarker', 'medium'])\n",
    "\n",
    "\n",
    "# N-grams: the probabilistic search space of our entities\n",
    "biomarker_ngrams = Ngrams(n_max=1)\n",
    "condition_ngrams = Ngrams(n_max=7)\n",
    "drug_ngrams = Ngrams(n_max=5)\n",
    "medium_ngrams = Ngrams(n_max=5)\n",
    "type_ngrams = Ngrams(n_max=5)  # <--- Q: should we cut these down?\n",
    "\n",
    "\n",
    "# Construct our Matchers\n",
    "bMatcher = matchers.getBiomarkerMatcher()\n",
    "cMatcher = matchers.getDiseaseMatcher()\n",
    "dMatcher = matchers.getDrugMatcher()\n",
    "mMatcher = matchers.getMediumMatcher()\n",
    "tMatcher = matchers.getTypeMatcher()\n",
    "\n",
    "\n",
    "# Building the CandidateExtractors\n",
    "candidate_extractor_BC = CandidateExtractor(\n",
    "    BiomarkerCondition, [biomarker_ngrams, condition_ngrams], [bMatcher, cMatcher])\n",
    "candidate_extractor_BD = CandidateExtractor(\n",
    "    BiomarkerDrug, [biomarker_ngrams, drug_ngrams], [bMatcher, dMatcher])\n",
    "candidate_extractor_BM = CandidateExtractor(\n",
    "    BiomarkerMedium, [biomarker_ngrams, medium_ngrams], [bMatcher, mMatcher])\n",
    "candidate_extractor_BT = CandidateExtractor(\n",
    "    BiomarkerType, [biomarker_ngrams, type_ngrams], [bMatcher, tMatcher])\n",
    "\n",
    "\n",
    "# List of Candidate Sets for each relation type: [train, dev, test]\n",
    "cands_BC = grabCandidates(candidate_extractor_BC, BiomarkerCondition)\n",
    "cands_BD = grabCandidates(candidate_extractor_BD, BiomarkerDrug)\n",
    "cands_BM = grabCandidates(candidate_extractor_BM, BiomarkerMedium)\n",
    "cands_BT = grabCandidates(candidate_extractor_BT, BiomarkerType)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
